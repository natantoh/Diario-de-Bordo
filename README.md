# Diario-de-Bordo

# Docker + PySpark 3.4.4 + Python 3.11

Este projeto fornece um cont√™iner Docker com:

- Spark 3.4.4 (Hadoop 3, Scala 2.12)
- Python 3.11
- PySpark 3.4.4
- (Opcional) Delta Lake

---

## üîß Como usar

1. Construa a imagem:
   ```bash
   docker build -t meu-pyspark-app .

Rode o cont√™iner:
docker run --rm meu-pyspark-app

Sa√≠da esperada:
+-----+-----+
| Nome|Idade|
+-----+-----+
|Alice|   30|
|  Bob|   25|
|Carol|   27|
|David|   35|
+-----+-----+

üìÅ Estrutura
Dockerfile ‚Üí define o cont√™iner

requirements.txt ‚Üí instala pyspark

script.py ‚Üí exemplo de uso e teste

‚ÑπÔ∏è Notas importantes
J√° tem Java 17 pr√©-instalado via apt.

O spark-submit funciona no terminal do container, caso queira testes adicionais:
docker run --rm -it meu-pyspark-app bash
spark-submit --version


Para rodar notebooks, use outra imagem ou adapte este Dockerfile.

---

## üõ† 6. **Comando final para rodar**

```bash
docker build -t meu-pyspark-app .
docker run --rm meu-pyspark-app


E para abrir o shell interativo dentro do container:
docker run --rm -it meu-pyspark-app bash
spark-submit --version
python3.11 script.py

üîö Resumo
ü§ñ Tudo automatizado em Docker, sem necessidade de instala√ß√µes manuais

üîÑ Ambiente replic√°vel e port√°til entre Windows/macOS/Linux

üìù Inclui documenta√ß√£o e ambiente com Spark + Python corretos

## üõ† 7. **Comando para rodar o docker ap√≥s o build da imagem**

O projeto est√° salvando no caminho data, por isso, ao terminar de rodar via docker, perde-se os dados.  Para rodar o docker continuar com o dado, podemos usar os comandos abaixo:

Ao usar o par√¢metro `-v` no `docker run`, voc√™ **mapeia a pasta de dados do container para o seu host**, garantindo que tudo que for salvo em processed dentro do container ficar√° dispon√≠vel (e persistente) na sua m√°quina, mesmo ap√≥s o container ser removido.

---

## Como fazer no Windows

Se estiver usando **PowerShell**:
```sh
docker run --rm -v ${PWD}/diario-de-bordo/data:/app/diario-de-bordo/data diario-de-bordo
```

Se estiver usando **CMD**:
```cmd
docker run --rm -v %cd%\diario-de-bordo\data:/app/diario-de-bordo/data diario-de-bordo
```

Se estiver usando **Git Bash**:
```sh
docker run --rm -v "$(pwd)/diario-de-bordo/data:/app/diario-de-bordo/data" diario-de-bordo
```

---

## O que acontece?

- O Kedro salva a tabela Delta em `/app/diario-de-bordo/data/processed` (dentro do container).
- Com o volume, tudo que for salvo ali aparece em processed na sua m√°quina.
- Voc√™ pode abrir, ler, copiar ou versionar a tabela Delta normalmente ap√≥s o pipeline rodar.


## Rodando sem Docker - Configura√ß√£o manual no Windows

Nesta sess√£o, ser√° apresentado o passo a passo para rodar sem Docker, documentando os passos feitos para rodar manualmente no Windows:

### Pr√©-requisitos:
- [x] Download do Spark 3.4.4 com Scala 2.12: [Apache Spark Downloads](https://spark.apache.org/downloads.html)
- [x] Download do Python 3.11.9: [Python Downloads](https://www.python.org/downloads/windows/)
- [x] Download do Hadoop 3.3.5/bin (Windows): [WinUtils](https://github.com/cdarlint/winutils)
- [x] Download do Java JDK 17 (17.0.12): [Oracle JDK](https://www.oracle.com/java/technologies/javase/jdk17-archive-downloads.html)

### Configura√ß√£o das Vari√°veis de Ambiente

Os valores dependem do local de instala√ß√£o. No meu caso:

| Vari√°vel        | Valor                              |
|-----------------|------------------------------------|
| `HADOOP_HOME`   | `C:\hadoop-3.3.5`                  |
| `JAVA_HOME`     | `C:\Program Files\Java\jdk-17`     |
| `PYSPARK_PYTHON`| `C:\Program Files\Python311\python.exe` |
| `PYTHON_HOME`   | `C:\Program Files\Python311`       |
| `SPARK_HOME`    | `C:\spark-3.4.4-bin-hadoop3`      |

### Configura√ß√£o do PATH (adicionar):
```
C:\Program Files\Python311\Scripts\
C:\Program Files\Python311\
%JAVA_HOME%\bin
%HADOOP_HOME%\bin
%SPARK_HOME%\bin
%USERPROFILE%\AppData\Roaming\Python\Python311\Scripts
C:\Users\natan\AppData\Roaming\Python\Python311\Scripts
```

### Instala√ß√£o dos Requirements

Navegue at√© a pasta contendo `requirements.txt` e execute:

```powershell
pip install -r requirements.txt
```

### Verifica√ß√£o da Instala√ß√£o

Para confirmar que tudo est√° configurado corretamente:

```powershell
python -c "import pyspark; print(pyspark.__version__)"
```
1. Rodar o comando abaixo:

   **Para Windows (PowerShell):**
   ```powershell
   $env:PYSPARK_SUBMIT_ARGS="--packages io.delta:delta-core_2.12:2.4.0 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog pyspark-shell"
   ```

   **Para Bash (Linux/macOS):**
   ```bash
   export PYSPARK_SUBMIT_ARGS="--packages io.delta:delta-core_2.12:2.4.0 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog pyspark-shell"
   ```

2. Rodar o comando abaixo (igual em todos os sistemas), o comando abaixo √© executado na mesma pasta em que est√° o pyproject.toml:

   ```bash
   python -m kedro run
   ```
